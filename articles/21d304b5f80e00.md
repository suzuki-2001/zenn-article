---
title: "MLflow Prompt Enginnering UIã§LLM-jpãªã©ã®éã‚µãƒãƒ¼ãƒˆãƒ­ãƒ¼ã‚«ãƒ«LLMã‚’åˆ©ç”¨ã™ã‚‹"
emoji: "ğŸ‘Œ"
type: "tech" # tech: æŠ€è¡“è¨˜äº‹ / idea: ã‚¢ã‚¤ãƒ‡ã‚¢
topics: ["mlflow", "huggingface","llm-jp"]
published: false
---

*æ³¨æ„æ›¸ã: æœ¬è¨˜äº‹ã®åŸ·ç­†ã«ã¯èª¤å­—è„±å­—ãªã©ã®æ–‡ç« æ ¡æ­£ä»¥å¤–ã« __ç”ŸæˆAIã‚’åˆ©ç”¨ã—ã¦ã„ã¾ã›ã‚“__ã€‚*

[MLflow](https://mlflow.org/)ã¯æ©Ÿæ¢°å­¦ç¿’ã®MLOpsã®ãŸã‚ã®OSSãƒ©ã‚¤ãƒ–ãƒ©ãƒªã§ã™ã€‚æœ€æ–°ã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã§ã¯ã€LLMã«é–¢ã™ã‚‹å®Ÿé¨“ç®¡ç†ã®æ©Ÿèƒ½ãŒéšæ™‚é–‹ç™ºãƒ»ãƒªãƒªãƒ¼ã‚¹ã•ã‚Œã¦ã„ã¾ã™ã€‚ä»Šå›ã¯ãã®ä¸­ã§ã‚‚Prompt Engineering UIã‚’ä½¿ã£ã¦ã€éã‚µãƒãƒ¼ãƒˆã®ãƒ­ãƒ¼ã‚«ãƒ«LLMã‚’åˆ©ç”¨ã™ã‚‹æ–¹æ³•ã‚’ç´¹ä»‹ã—ã¾ã™ã€‚

## MLflow Prompt Enginnering UIã¨ã¯ï¼Ÿ
[MLflow Prompt Enginnering UI](https://mlflow.org/docs/latest/llms/prompt-engineering/index.html)ã¯ã€ãƒãƒ¼ã‚³ãƒ¼ãƒ‰ã§æ§˜ã€…ãªãƒ¢ãƒ‡ãƒ«ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’è¡Œã„ã€ãã‚Œã‚‰ã‚’UIä¸Šã§ç°¡å˜ã«æ¯”è¼ƒã™ã‚‹ã“ã¨ãŒã§ãã‚‹æ©Ÿèƒ½ã§ã™ã€‚Prompt Engineering UIã‚’åˆ©ç”¨ã™ã‚‹ã“ã¨ã§ã€ãƒ¢ãƒ‡ãƒ«ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æœ€é©åŒ–ã™ã‚‹ãŸã‚ã®å®Ÿé¨“ã‚’ç°¡å˜ã«è¡Œã†ã“ã¨ãŒã§ãã¾ã™ã€‚

![](/images/mlflow_prompt/prompt_modal_2.png)
*MLflow Prompt Engineering UIã®æ“ä½œç”»é¢*

ã“ã®Prompt Enginnering UIã‚’åˆ©ç”¨ã™ã‚‹ãŸã‚ã«ã¯ã€[MLflow AI Gateway](https://mlflow.org/docs/latest/llms/deployments/index.html#providers)ã‹ã‚‰ãƒ¢ãƒ‡ãƒ«ã®æ¨è«–ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã‚’ä½œæˆã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚[OpenAI](https://openai.com/ja-JP/)ãªã©ã®ä¸»è¦ãªç”ŸæˆAIãƒ¢ãƒ‡ãƒ«ã¯å…¬å¼ã«ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ãŠã‚Šã€[OpenAI API Key](https://openai.com/index/openai-api/)ã¨yamlãƒ•ã‚¡ã‚¤ãƒ«ã‚’è¨­å®šã™ã‚‹ã ã‘ã§ã€ç°¡å˜ã«åˆ©ç”¨ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚

![](/images/mlflow_prompt/supported_providers.png)
*MLflow AI GatewayãŒå…¬å¼ã«ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ã‚‹ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼*

ä»Šå›ã¯ãã‚Œã‚‰ã®å…¬å¼ã‚µãƒãƒ¼ãƒˆãƒ¢ãƒ‡ãƒ«ã§ã¯ãªã„ã€[LLM-jp](https://llm-jp.nii.ac.jp/)ãªã©ã®ãƒ­ãƒ¼ã‚«ãƒ«ãƒ¢ãƒ‡ãƒ«ã‚’ç”¨ã„ã¦MLflow Prompt Enginnering UIã‚’åˆ©ç”¨ã™ã‚‹æ–¹æ³•ã‚’ç´¹ä»‹ã—ã¾ã™ã€‚

## Prompt Enginnering UIã§éã‚µãƒãƒ¼ãƒˆã®ãƒ­ãƒ¼ã‚«ãƒ«LLMã‚’åˆ©ç”¨ã™ã‚‹
MLflow Prompt Engineering UIã‚’åˆ©ç”¨ã—ã¦ã€Huggingfaceãƒ¢ãƒ‡ãƒ«ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’è¡Œã†æ–¹æ³•ã¯ä¸»ã«2ã¤ã‚ã‚Šã¾ã™ã€‚

1. [Huggingface Text Generation Interface (TGI)](https://huggingface.co/docs/text-generation-inference/index)

![](/images/mlflow_prompt/hf_tgi.png)
*Huggingface TGIã®æ¦‚è¦*

Huggingface TGIã¯ã€Rust/Python/gRPCã‚µãƒ¼ãƒãƒ¼ã‹ã‚‰æ§‹æˆã•ã‚Œã‚‹ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆãƒ„ãƒ¼ãƒ«ã‚­ãƒƒãƒˆã§ã™ã€‚2025å¹´2æœˆç¾åœ¨ã§ã¯ã€`Llama`ã‚„`Phi 3`, `Deepseek V3`ãªã©ã®ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹LLMã‚’ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ã¾ã™([ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ã‚‹ãƒ¢ãƒ‡ãƒ«ãƒªã‚¹ãƒˆ](https://huggingface.co/docs/text-generation-inference/supported_models))ã€‚

:::message

ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ãªã„ãƒ¢ãƒ‡ãƒ«ã‚‚åˆ©ç”¨å¯èƒ½ã§ã™ãŒã€ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã¯ä¿è¨¼ã•ã‚Œã¦ã„ãªã„ã‚ˆã†ã§ã™ã€‚

> If the above list lacks the model you would like to serve, depending on the modelâ€™s pipeline type, you can try to initialize and serve the model anyways to see how well it performs, but performance isnâ€™t guaranteed for non-optimized models

:::

ã¾ãŸå…¬å¼Dockerã‚³ãƒ³ãƒ†ãƒŠãŒå…¬é–‹ã•ã‚Œã¦ãŠã‚Šã€ä»¥ä¸‹ã®ã‚ˆã†ã«åˆ©ç”¨ã§ãã¾ã™ ([Quick Tour](https://huggingface.co/docs/text-generation-inference/quicktour))ã€‚NVIDIA GPUã‚’åˆ©ç”¨ã™ã‚‹å ´åˆã«ã¯[NVIDIA Container Toolkit](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html)ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ãŒå¿…è¦ã§ã™ã€‚ã¾ãŸã€CUDA 12.2ä»¥ä¸ŠãŒæ¨å¥¨ã•ã‚Œã¦ã„ã¾ã™ã€‚

```bash
model=teknium/OpenHermes-2.5-Mistral-7B
volume=$PWD/data # share a volume with the Docker container to avoid downloading weights every run

docker run --gpus all --shm-size 1g -p 8080:80 -v $volume:/data \
    ghcr.io/huggingface/text-generation-inference:3.1.0 \
    --model-id $model
```

[å…¬å¼ãƒªãƒã‚¸ãƒˆãƒª](https://github.com/huggingface/text-generation-inference)ã‹ã‚‰ãƒ­ãƒ¼ã‚«ãƒ«ãƒ“ãƒ«ãƒ‰ã‚‚å¯èƒ½ã§ã™ãŒã€Dockerã‚’åˆ©ç”¨ã™ã‚‹ã“ã¨ãŒå¼·ãæ¨å¥¨ã•ã‚Œã¦ã„ã‚‹ã‚ˆã†ã§ã™ã€‚è‘—è€…ã‚‚ãƒ­ãƒ¼ã‚«ãƒ«ãƒ“ãƒ«ãƒ‰ã‚’è©¦ã¿ã¾ã—ãŸãŒã€ãƒ“ãƒ«ãƒ‰ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚

TGIã®ãƒªã‚½ãƒ¼ã‚¹è¦ä»¶ã‚’æº€ãŸã™å ´åˆã«ã¯ã€ã“ã¡ã‚‰ã§æ¨è«–ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã‚’ä½œæˆã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚DockerãŒåˆ©ç”¨ã§ããªã„ç’°å¢ƒã‚„ã€mlflowãƒ©ã‚¤ãƒ–ãƒ©ãƒªã«çµ±ä¸€ã—ãŸã„å ´åˆã«ã¯ã€æ¬¡ã«ç´¹ä»‹ã™ã‚‹æ–¹æ³•ã‚‚åˆ©ç”¨å¯èƒ½ã§ã™ã€‚


2. [MLflow Model Serving](https://mlflow.org/docs/latest/deployment/deploy-model-locally.html)

Huggingface TGIã‚’åˆ©ç”¨ã§ããªã„å ´åˆã¯ã€Huggingfaceãƒ¢ãƒ‡ãƒ«ã‚’[pyfunc.PythonMode](https://mlflow.org/docs/latest/python_api/mlflow.pyfunc.html)ã§ãƒ©ãƒƒãƒ—ã—ã€MLflow Model Servingã‚’åˆ©ç”¨ã—ã¦æ¨è«–ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã‚’ä½œæˆã—ã¾ã™ã€‚

å…¨ä½“ã®æµã‚Œã¯æ¬¡ã®ã¨ãŠã‚Šã§ã™ã€‚
```mermaid
graph TD;
    A[Hugging Face Hub Model Card] -->|Convert to| B[MLflow PyFunc Model];
    B -->|Save Locally| C[Local MLflow Model Storage];
    C -->|Serve with MLflow| D[MLflow Model Serving];
    D -->|Expose via AI Gateway| E[MLflow AI Gateway Endpoint];
    E -->|Use in| F[MLflow Prompt Engineering UI];
```

ä»Šå›ã¯[LLM-jp-3-3.7b-instruct](https://huggingface.co/llm-jp/llm-jp-3-3.7b-instruct)ã‚’ä½¿ç”¨ã—ã¦ã¿ã¾ã™ã€‚ã¾ãšã€huggingfaceã‹ã‚‰ãƒ­ãƒ¼ãƒ‰ã—ãŸãƒ¢ãƒ‡ãƒ«ã‚’ã€`pyfunc.PythonModel`ã§ãƒ©ãƒƒãƒ—ã—ã€ãƒ­ãƒ¼ã‚«ãƒ«ã«ä¿å­˜ã—ã¾ã™ã€‚

```python
import torch
import mlflow.pyfunc
from transformers import AutoModelForCausalLM, AutoTokenizer


class HFTextGenModel(mlflow.pyfunc.PythonModel):
    def __init__(self, model_repo=None):
        self.model_repo = model_repo
        self.model = None
        self.tokenizer = None

    def load_context(self, context):
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.model = AutoModelForCausalLM.from_pretrained(self.model_repo).to(device)
        self.tokenizer = AutoTokenizer.from_pretrained(self.model_repo)

    def predict(self, context, model_input):
        prompts = self.tokenizer.apply_chat_template(model_input, tokenize=False)
        inputs = self.tokenizer(prompts, return_tensors="pt").to(self.model.device)

        with torch.inference_mode():
            output_ids = self.model.generate(
                inputs.input_ids,
                max_new_tokens=100,
                do_sample=True,
                top_p=0.95,
                temperature=0.7,
                repetition_penalty=1.05,
            )[0]

        generated_text = self.tokenizer.decode(output_ids, skip_special_tokens=True)

        return ["".join(generated_text)]


if __name__ == "__main__":
    save_dir = "./saved_model"
    model_repo = "llm-jp/llm-jp-3-3.7b-instruct"

    mlflow.pyfunc.save_model(
        path=save_dir,
        python_model=HFTextGenModel(model_repo),
        conda_env=None,
    )

```

ç¶šã„ã¦yamlãƒ•ã‚¡ã‚¤ãƒ«ã«æ¨è«–ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã®è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’è¨˜è¿°ã—ã¾ã™ã€‚æ¯”è¼ƒç”¨ã¨ã—ã¦OpenAIã®ãƒãƒ£ãƒƒãƒˆãƒ¢ãƒ‡ãƒ«(gpt-4, gpt-4o-mini)ã‚’è¿½åŠ ã—ã¦ã„ã¾ã™ã€‚
ã“ã“ã§providerã«mlflow-model-servingã‚’æŒ‡å®šã—ã€model_server_urlã«ãƒ­ãƒ¼ã‚«ãƒ«ã®ãƒ¢ãƒ‡ãƒ«ã‚µãƒ¼ãƒãƒ¼ã®URLã‚’æŒ‡å®šã—ã¾ã™ã€‚ã¾ãŸnameã«ã¯ã€ä»»æ„ã®ãƒ¢ãƒ‡ãƒ«åã‚’è¨˜è¿°ã—ã¾ã™ã€‚ã“ã®æŒ‡å®šãŒãªã„ã¨ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã™ã‚‹ã®ã§æ³¨æ„ã—ã¦ãã ã•ã„ã€‚

```yaml
endpoints:
    # OpenAI Chat
    - name: chat
    endpoint_type: llm/v1/chat
    model:
        provider: openai
        name: gpt-4
        config:
            openai_api_key: $OPENAI_API_KEY

    - name: chat_3.5
    endpoint_type: llm/v1/chat
    model:
        provider: openai
        name: gpt-4o-mini
        config:
            openai_api_key: $OPENAI_API_KEY


    # LLM-jp (Huggingface) 
    - name: llm-jp-3
    endpoint_type: llm/v1/chat
    model:
        provider: mlflow-model-serving
        name: llm-jp-3-3.7b-instruct
        config:
            model_server_url: "http://localhost:5001"
```

æœ€å¾Œã«ã€ä»¥ä¸‹ã®ã‚³ãƒãƒ³ãƒ‰ã‚’å®Ÿè¡Œã—ã¦MLflow Model Servingã¨MLflow AI Gatewayã‚’èµ·å‹•ã—ã¾ã™ã€‚`MLFLOW_DEPLOYMENTS_TARGET`ã«ã¯ã€MLflow AI Gatewayã®URLã‚’æŒ‡å®šã—ã¾ã™ã€‚ã‚‚ã—OpenAIãƒ¢ãƒ‡ãƒ«ã‚’åˆ©ç”¨ã™ã‚‹å ´åˆã¯ã€`OPENAI_API_KEY`ã«APIã‚­ãƒ¼ã‚’è¨­å®šã—ã¦ãã ã•ã„ã€‚


- MLflow Model Serving
```bash
mlflow models serve -m ./saved_model --no-conda --port 5001
```

- MLflow AI Gateway
```bash
export OPENAI_API_KEY="xxxxxxx" # if you use OpenAI Model
mlflow gateway start --config-path config.yaml --port 7000
```

- MLflow UI
```bash
export MLFLOW_DEPLOYMENTS_TARGET="http://127.0.0.1:7000"
mlflow server --port 5000 
```

ã“ã“ã¾ã§ã®æº–å‚™ãŒã§ããŸã‚‰ã€ãƒ–ãƒ©ã‚¦ã‚¶ã§`http://localhost:5000`ã«ã‚¢ã‚¯ã‚»ã‚¹ã—ã¦MLflow UIã‚’é–‹ãã€Prompt Engineering UIã‚’åˆ©ç”¨ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚

## LLM-jpã§Prompt Engineering UIã‚’åˆ©ç”¨ã—ã¦ã¿ã‚‹

ã“ã¡ã‚‰ãŒPrompt Engineering UIã®ç”»é¢ã§ã™ã€‚ã“ã“ã¾ã§æ¥ã‚Œã°ã€ãƒãƒ¼ã‚³ãƒ¼ãƒ‰ã§è¤‡æ•°ã®ãƒ¢ãƒ‡ãƒ«ã¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®çµ„ã¿åˆã‚ã›ã‚’æ¤œè¨¼ã—ã€ãã®è¨˜éŒ²ã‚’ä¿å­˜ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚

![](/images/mlflow_prompt/prompt_ui.png)

### å®Ÿéš›ã«è©¦ã—ã¦ã¿ã‚‹
ã¾ãšExperimentsã‚’ä½œæˆã—ã¾ã™ã€‚æ¬¡ã«ã€å³ä¸Šã«è¡¨ç¤ºã•ã‚Œã¦ã„ã‚‹`+ New run`ã‚’ã‚¯ãƒªãƒƒã‚¯ã—ã¾ã™ã€‚
ã™ã‚‹ã¨ã€`using Prompt Engineering (Experimental)`ã¨`using Notebook`ã®2ã¤ã®é¸æŠè‚¢ãŒè¡¨ç¤ºã•ã‚Œã¾ã™ã€‚ä»Šå›ã¯`using Prompt Engineering (Experimental)`ã‚’é¸æŠã—ã¾ã™ã€‚

![](/images/mlflow_prompt/new_run.png)
*æ–°ã—ã„Runã‚’ä½œæˆã™ã‚‹*

æ¬¡ã«ä»¥ä¸‹ã®ã‚ˆã†ãªç”»é¢ãŒè¡¨ç¤ºã•ã‚Œã¾ã™ã€‚

![](/images/mlflow_prompt/prompt_screen.png)
*Prompt Engineering UIã®ç”»é¢*

æ­£ã—ãæ¨è«–ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆãŒèªè­˜ã•ã‚Œã¦ã„ã‚Œã°ã€ç”»é¢å·¦ä¸Šã®`served LLM model`ã®ãƒˆã‚°ãƒ«ã‚’é–‹ãã¨ã€yamlãƒ•ã‚¡ã‚¤ãƒ«ã§æŒ‡å®šã—ãŸãƒ¢ãƒ‡ãƒ«ãŒè¡¨ç¤ºã•ã‚Œã¾ã™ã€‚ä»Šå›ã¯`llm-jp-3-3.7b-instruct`ã‚’é¸æŠã—ã¦ãŠãã¾ã™ã€‚

![](/images/mlflow_prompt/select_model.png)
*æ¨è«–ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã®é¸æŠ*

æ¬¡ã«ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å…¥åŠ›ã—ã¾ã™ã€‚Prompt Templateã§ã¯`{{}}`ã‚’ä½¿ç”¨ã—ã¦å¤‰æ•°ã‚’æŒ‡å®šã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚ä»Šå›ã¯`{{object}}`ã¯ä½•è‰²ã§ã™ã‹ï¼Ÿã¨ã„ã†ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’ä½œæˆã—ã€`object`ã«`ã‚Šã‚“ã”`ã‚’æŒ‡å®šã—ã¦ã€evaluateãƒœã‚¿ãƒ³ã‚’ã‚¯ãƒªãƒƒã‚¯ã—ã¾ã™ã€‚
ã™ã‚‹ã¨Outputãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã«ç”Ÿæˆã•ã‚ŒãŸæ–‡ç« ãŒè¡¨ç¤ºã•ã‚Œã¾ã™ã€‚ä»Šå›ç”Ÿæˆã•ã‚ŒãŸæ–‡ç« ãŒã€è¿”ç­”ã¨ã—ã¦ãŠã‹ã—ã„ã‚ˆã†ã«è¦‹ãˆã¾ã™ãŒã€ãã®åŸå› ã¯å¾Œã»ã©èª¬æ˜ã—ã¾ã™ã€‚
(`llm-jp-3.7b-instruct`ã«åŒã˜ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’åˆ¥é€”ä¸ãˆã‚‹ã¨ã€ãã¡ã‚“ã¨ã‚Šã‚“ã”ãŒèµ¤è‰²ã§ã‚ã‚‹ã¨ã„ã†æ—¨ã®å‡ºåŠ›ã‚’ã™ã‚‹ã®ã§ã€MLflowã®å…¥åŠ›ã«å•é¡ŒãŒã‚ã‚‹å¯èƒ½æ€§ãŒé«˜ã„ã§ã™ã€‚)

![](/images/mlflow_prompt/ja_prompt.png)
*æ—¥æœ¬èªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å…¥åŠ›ã—ã¦ã¿ã‚‹*

å³ä¸‹ã®`create run`ãƒœã‚¿ãƒ³ã‚’ã‚¯ãƒªãƒƒã‚¯ã™ã‚‹ã¨ã€ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®è©•ä¾¡çµæœãŒä¿å­˜ã•ã‚Œã¾ã™ã€‚ä¿å­˜ã•ã‚ŒãŸçµæœã¯ã€Experimentsã®`Evaluation (Experimental)`ã‚¿ãƒ–ã§ç¢ºèªã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚
ã“ã“ã§ã¯ã€è¤‡æ•°ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¨ãƒ¢ãƒ‡ãƒ«ã‚’æ¯”è¼ƒã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚

![](/images/mlflow_prompt/compare_results.png)
*è¤‡æ•°ã®ãƒ¢ãƒ‡ãƒ«ã¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æ¯”è¼ƒã™ã‚‹*


### å•é¡Œç‚¹
ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æ—¥æœ¬èªã§å…¥åŠ›ã™ã‚‹ã¨ã€å†…éƒ¨ã§æ–‡å­—åŒ–ã‘ã—ã¦ã—ã¾ã†ã‚ˆã†ã§ã™ã€‚[ã“ã¡ã‚‰](https://qiita.com/isanakamishiro2/items/184f7ee8a1d00cc01fea)ã®è¨˜äº‹ã§ã‚‚MLflowã®æ–‡å­—åŒ–ã‘ãŒå ±å‘Šã•ã‚Œã¦ã„ã¾ã™ã€‚

![](/images/mlflow_prompt/ja_prompt_artifact.png)
*æ—¥æœ¬èªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®æ–‡å­—åŒ–ã‘*

![](/images/mlflow_prompt/en_prompt_artifact.png)
*è‹±èªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ (stock_type template)*

ç”»åƒã§ã¯OpenAIãƒ¢ãƒ‡ãƒ«ã®å‡ºåŠ›ã¯æ­£å¸¸ã§ã™ãŒã€æ–‡å­—åŒ–ã‘ã—ãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãŒæ¸¡ã•ã‚Œã¦ã„ã¾ã™ã€‚ã“ã‚Œã‚‰ã®ãƒ¢ãƒ‡ãƒ«ã§ã¯ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãŒæ–‡å­—åŒ–ã‘ã—ã¦ã„ã¦ã‚‚ã€å…ƒã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ–‡ç« ã‚’å¾©å…ƒã§ãã‚‹ã‚ˆã†ã§ã™ã€‚

:::message

MLflowå…¬å¼ãƒªãƒã‚¸ãƒˆãƒªã«Issueã‚’å ±å‘Šã—ã¦ã„ã¾ã™ã€‚

:::

## å‚è€ƒè³‡æ–™

- [LLM-jp](https://llm-jp.nii.ac.jp/)
- [MLflow Prompt Enginnering UI](https://mlflow.org/docs/latest/llms/prompt-engineering/index.html)
- [MLflow Model Serving](https://mlflow.org/docs/latest/deployment/deploy-model-locally.html)
- [MLflow AI Gateway](https://mlflow.org/docs/latest/llms/deployments/index.html#providers)
- [Huggingface Text Generation Interface (TGI)](https://huggingface.co/docs/text-generation-inference/index)

